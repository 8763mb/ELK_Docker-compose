# ./logstash/pipeline/kafka-demo.conf
input {
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["kafka-demo"]
    group_id => "logstash-demo"
    auto_offset_reset => "earliest"
    codec => "json"

    # 防止訊息丟失：調整 Kafka consumer 配置
    # 參考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-kafka.html

    # 增加 session timeout 給 ES 寫入更多時間
    session_timeout_ms => "30000"

    # 減少每次 poll 的記錄數，降低處理壓力
    max_poll_records => "100"

    # 增加心跳間隔
    heartbeat_interval_ms => "3000"

    # 設置 consumer threads
    consumer_threads => 1
  }
}

# filter {
#   # 你的 JSON 是 "timestamp"，轉成 ES 可用的 @timestamp
#   date {
#     match  => ["timestamp", "ISO8601"]
#     target => "@timestamp"
#   }
# 
#   # 可選：統一欄位命名，方便 Kibana/Elastic 方案
#   mutate {
#     rename => { "service" => "service.name" }
#     rename => { "level"   => "log.level" }
#   }
# }

output {
  # 傳到 Elasticsearch（HTTP 有開 TLS）
  elasticsearch {
    hosts    => ["https://elasticsearch:9200"]
    user     => "elastic"
    password => "${ELASTIC_PASSWORD}"
    ssl_enabled => true
    ssl_certificate_authorities => "/usr/share/logstash/config/certs/ca.crt"
    index    => "kafka-demo-%{+YYYY.MM.dd}"
  }

  # 除錯輸出（可觀察 Logstash 端是否有吃到資料）
  stdout { codec => rubydebug }
}
